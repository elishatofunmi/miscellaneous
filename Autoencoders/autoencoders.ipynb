{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders are artificial networks capable of learning efficient representation of the input data, called codings, without any supervision (i.e., the training set is unlabeled). \n",
    "\n",
    "Autoencoders act as powerful feature detectors, and they can be used for unsupervised pretraining of deep neural networks.\n",
    "\n",
    "They are capable of randomly generating new data that looks  very similar to the training data; this is called a generative model.\n",
    "\n",
    "for example, I can train an autoencoder on pictures of faces, and it would then be able to generate new faces.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders work by simply learning to copy their inputs to their outputs. For example, you can limit the size of the internal representation, or you can add noise to the inputs and train the network to recover the original inputs. These constraints prevent the autoencoder from trivially copying the inputs directly to the outputs, which forces it to learn efficient ways of representing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can be used for dimensionality reduction, feature extraction, unsupervised pretraining or as generative models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An autoencoder is always composed of two parts 'an encoder' (or recognition network) that converts the inputs to an internal representation, followed by 'a decoder' (or generative network) that converts the internal representation to the outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In autoencoders, the number of neurons in the output layer must be equal to the number of inputs. The output layer are often called the reconstruction since the autoencoder tries to reconstruct the inputs, and the cost function contains a reconstruction loss that penalizes the model when the reconstructions are different from the inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# performing PCA with an undercomplete Linear Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.compat.v1.layers import fully_connected\n",
    "n_inputs = 3 #3D inputs\n",
    "n_hidden = 2 #2D codings\n",
    "\n",
    "n_outputs = n_inputs\n",
    "learning_rate = 0.01\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "hidden = fully_connected(X, n_hidden, activation_fn = None)\n",
    "outputs = fully_connected(hidden, n_outputs, activation_fn = None)\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(output - x)) # MSE\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(reconstruction_loss)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code isn't differnt from all MLPs we built in the past chapters. The two things to note are:\n",
    "-The number of outputs is equal to the number of inputs.\n",
    "-To perform simple PCA, we set activation_fn = None (i.e., all neurons are linear) and the cost function is the MSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test = [.....]# load the dataset\n",
    "\n",
    "codings = hidden # the output of the hidden layer provides the coding.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for iteration in range(n_iterations):\n",
    "        training_op.run(feed_dict={X:X_train}) # no labels\n",
    "    coding_val = codings.eval(feed_dict = {X:X_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoencoders can have multiple hidden layers. In this case, they are called stacked Autoencoders (or deep autoencoders). Adding more layers helps the autoencoder learn more complex coding. \n",
    "\n",
    "Tensorflow implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28\n",
    "n_hideen1 = 300\n",
    "n_hidden2 = 150\n",
    "n_hidden3 = n_hidden1\n",
    "n_outputs = n_outputs\n",
    "\n",
    "learning_rate = 0.01\n",
    "l2_reg = 0.001\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None, n_inputs])\n",
    "\n",
    "with tf.contrib.framework.arg_scope([fully_connected],activation = tf.nn.elu,\n",
    "                                    weights_initializer = tf.contrib.layers.variance_scaling_initializer(),\n",
    "                                    weights_regualarizer = tf.contrib.layers.l2_regularizer(l2_reg)):\n",
    "    \n",
    "    hidden1 = fully_connected(X, n_hidden1)\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2)\n",
    "    hidden3 = fully_connected(hidden2, n_hidden3)\n",
    "    outputs = fully_connected(hidden3, n_outputs, activation_fn = None)\n",
    "    \n",
    "reconstruction_loss = tf.reduce_mean(tf.square(outputs - X)) # MSE\n",
    "\n",
    "reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "loss = tf.add_n([reconstruction_loss]+reg_losses)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size = 150\n",
    "\n",
    "with tf.session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        n_batches = mnist.train.num_examples //batch_size\n",
    "        for iteration in  range(n_batches):\n",
    "            x_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run(training_op, feed_dict = {X:x_batch})\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tying weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This halves the number of weights in the model, speeding up training and limiting the risk of overfitting. Specifically, if the autoencoder has a total of N layers (not counting the input layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = tf.nn.elu\n",
    "regularizer = tf.contrib.layers.l2_regularizer(l2_reg)\n",
    "initializer = tf.contrib.layers.variance_scaling_initializer()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape = [None,n_inputs])\n",
    "\n",
    "weights1_init = intializer([n_inputs, n_hidden1])\n",
    "weights2_init = initializer([n_hidden1, n_hidden2])\n",
    "\n",
    "weights1 = tf.Variable(weights1_init, dtype = tf.float32, name = 'weights1')\n",
    "weights2 = tf.Variable(weights2_init, dtype = tf.float32, name = 'weights2')\n",
    "weights3 = tf.transpose(weights2, name = 'weights3') # tied weights\n",
    "weights4 = tf.transpose(weights1, name = 'weights4') # tied weights\n",
    "\n",
    "biases1 = tf.Variable(tf.zeros(n_hidden1), name = 'biases1')\n",
    "biases2 = tf.Variable(tf.zeros(n_hidden2), name = 'biases2')\n",
    "biases3 = tf.Variable(tf.zeros(n_hidden3), name = 'biases3')\n",
    "biases4 = tf.Variable(tf.zeros(n_outputs), name = 'biases4')\n",
    "\n",
    "hidden1 = activation(tf.matmul(X, weights1) + biases1)\n",
    "hidden2 = activation(tf.matmul(hidden1, weights2) + biases2)\n",
    "hidden3 = activation(tf.matmul(hidden2, weights3) + biases3)\n",
    "output = tf.matmul(hidden3, weights4) + biases4\n",
    "\n",
    "reconstruction_loss = tf.reduce_mean(tf.square(output = X))\n",
    "reg_loss = regularizer(weights1) + regularizer(weights)\n",
    "\n",
    "loss = reconstruction_loss + reg_loss\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
