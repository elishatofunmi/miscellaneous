{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of prepare text data with sklearn","provenance":[{"file_id":"1MC-dwr4ULHtq_gHMv7SEntNt_XCZakP6","timestamp":1590031522698}],"authorship_tag":"ABX9TyN0neCV/wBs5hmD3NPJvx3O"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"VVu7tvw-PcA8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"da300bd9-85b9-4953-c6e9-bced05ab26b1","executionInfo":{"status":"ok","timestamp":1590062704646,"user_tz":0,"elapsed":2655,"user":{"displayName":"Elisha Odemakinde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJMGBZrfhncib9KtMhT4txIPreuxknzXr28ZCu7A=s64","userId":"06070849107023429198"}}},"source":["#word countt with countVectorizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","text = ['The quick brown fox jumped over the lazy dog.']\n","vectorizer = CountVectorizer()\n","vectorizer.fit(text)\n","print(vectorizer.vocabulary_)\n","vector = vectorizer.transform(text)\n","print(vector.shape)\n","print(type(vector))\n","print(vector.toarray())"],"execution_count":1,"outputs":[{"output_type":"stream","text":["{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n","(1, 8)\n","<class 'scipy.sparse.csr.csr_matrix'>\n","[[1 1 1 1 1 1 1 2]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Rr8XS3rMSvXQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"6c1af9c7-54ec-40ef-cb7f-b0447e73d2cf","executionInfo":{"status":"ok","timestamp":1590062704654,"user_tz":0,"elapsed":2622,"user":{"displayName":"Elisha Odemakinde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJMGBZrfhncib9KtMhT4txIPreuxknzXr28ZCu7A=s64","userId":"06070849107023429198"}}},"source":["# the tfidf vectorizerr will tokenize documents, learn the vocabulary and inverse document frequencyy \n","# weightings, and allow you to encode new documents.\n","\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer()\n","text = ['The quick brown fox jumped over the lazy dog.']\n","\n","vectorizer.fit(text)\n","print(vectorizer.vocabulary_)\n","print(vectorizer.idf_)\n","\n","vector = vectorizer.transform(text)\n","print(vector.shape)\n","print(vector.toarray())\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n","[1. 1. 1. 1. 1. 1. 1. 1.]\n","(1, 8)\n","[[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n","  0.30151134 0.60302269]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5oYeRyVZTbYG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"1d7fbb01-6224-4321-cee6-0f4af53fb11a","executionInfo":{"status":"ok","timestamp":1590062704660,"user_tz":0,"elapsed":2598,"user":{"displayName":"Elisha Odemakinde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJMGBZrfhncib9KtMhT4txIPreuxknzXr28ZCu7A=s64","userId":"06070849107023429198"}}},"source":["#hashing with hashing vectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","vectorizer = HashingVectorizer(n_features = 20)\n","vector = vectorizer.transform(text)\n","print(vector.shape)\n","print(vector.toarray())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["(1, 20)\n","[[ 0.          0.          0.          0.          0.          0.33333333\n","   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n","   0.          0.          0.         -0.33333333  0.          0.\n","  -0.66666667  0.        ]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pB88pUQEUSxV","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}