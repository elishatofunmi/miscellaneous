{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"language model2.ipynb","provenance":[],"authorship_tag":"ABX9TyPQF3TQ6Oasl8kIZCJraJux"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e-zkL868UBWk","colab_type":"text"},"source":["# sentimental classification using RNN"]},{"cell_type":"code","metadata":{"id":"2aLGv6ahUEXN","colab_type":"code","colab":{}},"source":["# Build and compile the model\n","model = Sequential()\n","model.add(Embedding(vocabulary_size, wordvec_dim, trainable=True, input_length=max_text_len))\n","model.add(LSTM(64, return_sequences=64, dropout=0.2, recurrent_dropout=0.15))\n","model.add(LSTM(64, return_sequences=False, dropout=0.2, recurrent_dropout=0.15))\n","model.add(Dense(16))\n","model.add(Dropout(rate=0.25))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Load pre-trained weights\n","model.load_weights('model_weights.h5')\n","\n","# Print the obtained loss and accuracy\n","print(\"Loss: {0}\\nAccuracy: {1}\".format(*model.evaluate(X_test, y_test, verbose=0)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WCEScUPHUin2","colab_type":"text"},"source":["# adding CNN layers"]},{"cell_type":"code","metadata":{"id":"ayxrkKV-UVMa","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":237},"outputId":"fd87c6b7-c58b-4e4c-d39a-11035d845041","executionInfo":{"status":"error","timestamp":1589764068514,"user_tz":0,"elapsed":2704,"user":{"displayName":"Elisha Odemakinde","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiJMGBZrfhncib9KtMhT4txIPreuxknzXr28ZCu7A=s64","userId":"06070849107023429198"}}},"source":["# Print the model summary\n","model_cnn.summary()\n","\n","# Load pre-trained weights\n","model_cnn.load_weights('model_weights.h5')\n","\n","# Evaluate the model to get the loss and accuracy values\n","loss, acc = model_cnn.evaluate(x_test, y_test, verbose=0)\n","\n","# Print the loss and accuracy obtained\n","print(\"Loss: {0}\\nAccuracy: {1}\".format(loss, acc))"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-cc8d7894cefa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Print the model summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Load pre-trained weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_cnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_weights.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model_cnn' is not defined"]}]},{"cell_type":"code","metadata":{"id":"M-WCKEfIUVSE","colab_type":"code","colab":{}},"source":["# Get the numerical ids of column label\n","numerical_ids = df.label.cat.codes\n","\n","# Print initial shape\n","print(numerical_ids.shape)\n","\n","# One-hot encode the indexes\n","Y = to_categorical(numerical_ids)\n","\n","# Check the new shape of the variable\n","print(Y.shape)\n","\n","# Print the first 5 rows\n","print(Y[:5])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ut-e-mTHUVZt","colab_type":"code","colab":{}},"source":["# Create and fit tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(news_dataset.data)\n","\n","# Prepare the data\n","prep_data = tokenizer.texts_to_sequences(news_dataset.data)\n","prep_data = pad_sequences(prep_data, maxlen=200)\n","\n","# Prepare the labels\n","prep_labels = to_categorical(news_dataset.target)\n","\n","# Print the shapes\n","print(prep_data.shape)\n","print(prep_labels.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9F9NyjD6Kyae","colab_type":"text"},"source":["Transfer learning starting point\n","\n","In this exercise you will see the benefit of using pre-trained vectors as a starting point for your model.\n","\n","You will compare the accuracy of two models trained with two epochs. The architecture of the models is the same: One embedding layer, one LSTM layer with 128 units and the output layer with 5 units which is the number of classes in the sample data. The difference is that one model uses pre-trained vectors on the embedding layer (transfer learning) and the other doesn't.\n","\n","The pre-trained vectors used were the GloVE with 200 dimension. The training accuracy history of the validation set of both models are available in the variables history_no_emb and history_emb."]},{"cell_type":"code","metadata":{"id":"mLJDqCvpUVW6","colab_type":"code","colab":{}},"source":["# Import plotting package\n","import matplotlib.pyplot as plt\n","\n","# Insert lists of accuracy obtained on the validation set\n","plt.plot(history_no_emb['acc'], marker='o')\n","plt.plot(history_emb['acc'], marker='o')\n","\n","# Add extra descriptions to plot\n","plt.title('Learning with and without pre-trained embedding vectors')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['no_embeddings', 'with_embeddings'], loc='upper left')\n","\n","# Display the plot\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vV8lLBQ-LRWj","colab_type":"text"},"source":["Word2Vec\n","\n","In this exercise you will create a Word2Vec model using Keras.\n","\n","The corpus used to pre-train the model is the script of all episodes of the The Big Bang Theory TV show, divided sentence by sentence. It is available in the variable bigbang.\n","\n","The text on the corpus was transformed to lower case and all words were tokenized. The result is stored in the tokenized_corpus variable.\n","\n","A Word2Vec model was pre-trained using a window size of 10 words for context (5 before and 5 after the center word), words with less than 3 occurrences were removed and the skip gram model method was used with 50 dimension. The model is saved on the file bigbang_word2vec.model.\n","\n","The class Word2Vec is already loaded in the environment from gensim.models.word2vec."]},{"cell_type":"code","metadata":{"id":"0aKRZUkKLSCM","colab_type":"code","colab":{}},"source":["# Word2Vec model\n","w2v_model = Word2Vec.load('bigbang_word2vec.model')\n","\n","# Selected words to check similarities\n","words_of_interest = ['bazinga', 'penny', 'universe', 'spock','brain']\n","\n","# Compute top 5 similar words for each of the words of interest\n","top5_similar_words = []\n","for word in words_of_interest:\n","    top5_similar_words.append(\n","      {word: [item[0] for item in w2v_model.wv.most_similar([word], topn=5)]}\n","    )\n","\n","# Print the similar words\n","print(top5_similar_words)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BlxRiJrbNBfj","colab_type":"text"},"source":["Exploring 20 News Groups dataset\n","\n","In this exercise, you will be given a sample of the 20 News Groups dataset obtained using the fetch_20newsgroups() function from sklearn.datasets, filtering only three classes: sci.space, alt.atheism and soc.religion.christian.\n","\n","The dataset is loaded in the variable news_dataset. Its attributes are printed so you can explore them on the console.\n","\n","Fore more details on how to use this function, see the Sklearn documentation.\n","\n","You will tokenize the texts and one-hot encode the labels step by step to understand how the transformations happen."]},{"cell_type":"code","metadata":{"id":"EIUhq6L4NB0x","colab_type":"code","colab":{}},"source":["# See example article\n","print(news_dataset.data[5])\n","\n","# Transform the text into numerical indexes\n","news_num_indices = tokenizer.texts_to_sequences(news_dataset.data)\n","\n","# Print the transformed example article\n","print(news_num_indices[5])\n","\n","# Transform the labels into one-hot encoded vectors\n","labels_onehot = to_categorical(news_dataset.target)\n","\n","# Check before and after for the sample article\n","print(\"Before: {0}\\nAfter: {1}\".format(news_dataset.target[5], labels_onehot[5]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IXq2YI0JPsgb","colab_type":"text"},"source":["Classifying news articles\n","\n","In this exercise you will create a multi-class classification model.\n","\n","The dataset is already loaded in the environment as news_novel. Also, all the pre-processing of the training data is already done and tokenizer is also available in the environment.\n","\n","A RNN model was pre-trained with the following architecture: use the Embedding layer, one LSTM layer and the output Dense layer expecting three classes: sci.space, alt.atheism, and soc.religion.christian. The weights of this trained model are available on the classify_news_weights.h5 file.\n","\n","You will pre-process the novel data and evaluate on a new dataset news_novel."]},{"cell_type":"code","metadata":{"id":"13IadSluXWWh","colab_type":"code","colab":{}},"source":["# Change text for numerical ids and pad\n","X_novel = tokenizer.texts_to_sequences(news_novel.data)\n","X_novel = pad_sequences(X_novel, maxlen=400)\n","\n","# One-hot encode the labels\n","Y_novel = to_categorical(news_novel.target)\n","\n","# Load the model pre-trained weights\n","model.load_weights('classify_news_weights.h5')\n","\n","# Evaluate the model on the new dataset\n","loss, acc = model.evaluate(X_novel, Y_novel, batch_size=64)\n","\n","# Print the loss and accuracy obtained\n","print(\"Loss:\\t{0}\\nAccuracy:\\t{1}\".format(loss, acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fjW8hpuVZFit","colab_type":"code","colab":{}},"source":["# Get probabilities for each class\n","pred_probabilities = model.predict_proba(X_test)\n","\n","# Thresholds at 0.5 and 0.8\n","y_pred_50 = [np.argmax(x) if np.max(x) >= 0.5 else DEFAULT_CLASS for x in pred_probabilities]\n","y_pred_80 = [np.argmax(x) if np.max(x) >= 0.8 else DEFAULT_CLASS for x in pred_probabilities]\n","\n","trade_off = pd.DataFrame({\n","    'Precision_50': precision_score(y_true, y_pred_50, average=None), \n","    'Precision_80': precision_score(y_true, y_pred_80, average=None), \n","    'Recall_50': recall_score(y_true, y_pred_50, average=None), \n","    'Recall_80': recall_score(y_true, y_pred_80, average=None)}, \n","  index=['Class 1', 'Class 2', 'Class 3'])\n","\n","print(trade_off)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpfqGMmRe1iz","colab_type":"text"},"source":["Text generation examples\n","\n","In this exercise, you are going to experiment on two pre-trained models for text generation.\n","\n","The first model will generate one phrase based on the character Sheldon of The Big Bang Theory TV show, and the second model will generate a Shakespeare poems up to 400 characters.\n","\n","The models are loaded on the sheldon_model and poem_model variables. Also, two custom functions to help generate text are available: generate_sheldon_phrase() and generate_poem(). Both receive the pre-trained model and a context string as parameters."]},{"cell_type":"code","metadata":{"id":"sCwRxP8jZGqc","colab_type":"code","colab":{}},"source":["# Context for Sheldon phrase\n","sheldon_context = \"Iâ€™m not insane, my mother had me tested. \"\n","\n","# Generate one Sheldon phrase\n","sheldon_phrase = generate_sheldon_phrase(sheldon_model, sheldon_context)\n","\n","# Print the phrase\n","print(sheldon_phrase)\n","\n","# Context for poem\n","poem_context = \"May thy beauty forever remain\"\n","\n","# Print the poem\n","print(generate_poem(poem_model, poem_context))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_0_B2VFgZGvF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}