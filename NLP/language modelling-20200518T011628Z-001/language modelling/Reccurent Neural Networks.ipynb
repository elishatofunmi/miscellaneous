{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Reccurent Neural Networks.ipynb","provenance":[],"authorship_tag":"ABX9TyObZnrk4ClUSMBijs16UQBd"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"IgzkiJeM2DiU","colab_type":"code","colab":{}},"source":["# Import relevant classes/functions\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","\n","# Build the dictionary of indexes\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(texts)\n","\n","# Change texts into sequence of indexes\n","texts_numeric = tokenizer.texts_to_sequences(texts)\n","print(\"Number of words in the sample texts: ({0}, {1})\".format(len(texts_numeric[0]), len(texts_numeric[1])))\n","\n","# Pad the sequences\n","texts_pad = pad_sequences(texts_numeric, 60)\n","print(\"Now the texts have fixed length: 60. Let's see the first one: \\n{0}\".format(texts_pad[0]))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"UagUWxrcJ36c","colab_type":"code","colab":{}},"source":["# Build model\n","model = Sequential()\n","model.add(SimpleRNN(units=128, input_shape=(None, 1)))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', \n","              optimizer='adam',\n","              metrics=['accuracy'])\n","\n","# Load pre-trained weights\n","model.load_weights('model_weights.h5')\n","\n","# Method '.evaluate()' shows the loss and accuracy\n","loss, acc = model.evaluate(x_test, y_test, verbose=0)\n","print(\"Loss: {0} \\nAccuracy: {1}\".format(loss, acc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shekhBeeMRzN","colab_type":"text"},"source":["# vanishing/exploding gradient\n"]},{"cell_type":"code","metadata":{"id":"Iw1C1sx0KSxI","colab_type":"code","colab":{}},"source":["# Create a Keras model with one hidden Dense layer\n","model = Sequential()\n","model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n","model.add(Dense(1, activation='linear'))\n","\n","# Compile and fit the model\n","model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n","\n","# See Mean Square Error for train and test data\n","train_mse = model.evaluate(X_train, y_train, verbose=0)\n","test_mse = model.evaluate(X_test, y_test, verbose=0)\n","\n","# Print the values of MSE\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wGgefbJjM2bq","colab_type":"text"},"source":["# solution via clipvalue in SGD (gradient clipping)"]},{"cell_type":"code","metadata":{"id":"hrKuYk5xKejw","colab_type":"code","colab":{}},"source":["# Create a Keras model with one hidden Dense layer\n","model = Sequential()\n","model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer=he_uniform(seed=42)))\n","model.add(Dense(1, activation='linear'))\n","\n","# Compile and fit the model\n","model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9, clipvalue=3.0))\n","history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, verbose=0)\n","\n","# See Mean Square Error for train and test data\n","train_mse = model.evaluate(X_train, y_train, verbose=0)\n","test_mse= model.evaluate(X_test, y_test, verbose=0)\n","\n","# Print the values of MSE\n","print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QjXDG6jCNLmQ","colab_type":"text"},"source":["# vanishing gradient \n"]},{"cell_type":"markdown","metadata":{"id":"KaBgJepdNL-P","colab_type":"text"},"source":["The other possible gradient problem is when the gradients vanish, or go to zero. This is a much harder problem to solve because it is not as easy to detect. If the loss function does not improve on every step, is it because the gradients went to zero and thus didn't update the weights? Or is it because the model is not able to learn?"]},{"cell_type":"code","metadata":{"id":"0llyVS8tKevX","colab_type":"code","colab":{}},"source":["# Create the model\n","model = Sequential()\n","model.add(SimpleRNN(units=600, input_shape=(None, 1)))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n","\n","# Load pre-trained weights\n","model.load_weights('model_weights.h5')\n","\n","# Plot the accuracy x epoch graph\n","plt.plot(history.history['acc'])\n","plt.plot(history.history['val_acc'])\n","plt.legend(['train', 'val'], loc='upper left')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xwPCqhI0Nk3w","colab_type":"text"},"source":["# GRU and LSTM cells (solution to vanishing gradient problem)"]},{"cell_type":"code","metadata":{"id":"ww9a8A5bKesR","colab_type":"code","colab":{}},"source":["# Import the modules\n","from keras.layers import GRU, Dense\n","\n","# Print the old and new model summaries\n","SimpleRNN_model.summary()\n","gru_model.summary()\n","\n","# Evaluate the models' performance (ignore the loss value)\n","_, acc_simpleRNN = SimpleRNN_model.evaluate(X_test, y_test, verbose=0)\n","_, acc_GRU = gru_model.evaluate(X_test, y_test, verbose=0)\n","\n","# Print the results\n","print(\"SimpleRNN model's accuracy:\\t{0}\".format(acc_simpleRNN))\n","print(\"GRU model's accuracy:\\t{0}\".format(acc_GRU))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1usEm7qtKeoz","colab_type":"code","colab":{}},"source":["# Import the LSTM layer\n","from keras.layers.recurrent import LSTM\n","\n","# Build model\n","model = Sequential()\n","model.add(LSTM(units=128, input_shape=(None, 1), return_sequences=True))\n","model.add(LSTM(units=128, return_sequences=True))\n","model.add(LSTM(units=128, return_sequences=False))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Load pre-trained weights\n","model.load_weights('lstm_stack_model_weights.h5')\n","\n","print(\"Loss: %0.04f\\nAccuracy: %0.04f\" % tuple(model.evaluate(X_test, y_test, verbose=0)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PhImSD8qR2FV","colab_type":"text"},"source":["# word embedding"]},{"cell_type":"markdown","metadata":{"id":"aujc7cENRwb5","colab_type":"text"},"source":["You saw that the one-hot representation is not a good representation of words because it is very sparse. Using the Embedding layer creates a dense representation of the vectors, but also demands a lot of parameters to be learned.\n","\n","In this exercise you will compare the number of parameters of two models using embeddings and one-hot encoding to see the difference.\n","\n","The model model_onehot is already loaded in the environment, as well as the Sequential, Dense and GRU from keras. Finally, the parameters vocabulary_size=80000 and sentence_len=200 are also loaded."]},{"cell_type":"code","metadata":{"id":"S8oWnV-TP2fm","colab_type":"code","colab":{}},"source":["# Import the embedding layer\n","from keras.layers import Embedding\n","\n","# Create a model with embeddings\n","model = Sequential(name=\"emb_model\")\n","model.add(Embedding(input_dim=80002, output_dim=wordvec_dim, input_length=200, trainable=True))\n","model.add(GRU(128))\n","model.add(Dense(1))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Print the summaries of the one-hot model\n","model_onehot.summary()\n","\n","# Print the summaries of the model with embeddings\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g6OlZ8FESj-r","colab_type":"text"},"source":["# Transfer learning"]},{"cell_type":"code","metadata":{"id":"QB7FbaUhP2xW","colab_type":"code","colab":{}},"source":["# Load the glove pre-trained vectors\n","glove_matrix = load_glove('glove_200d.zip')\n","\n","# Create a model with embeddings\n","model = Sequential(name=\"emb_model\")\n","model.add(Embedding(input_dim=vocabulary_size + 1, output_dim=wordvec_dim, \n","                    embeddings_initializer=Constant(glove_matrix), \n","                    input_length=sentence_len, trainable=False))\n","model.add(GRU(128))\n","model.add(Dense(1))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Print the summaries of the model with embeddings\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Oa4f4FPSzNP","colab_type":"text"},"source":["# embedding improves performances"]},{"cell_type":"code","metadata":{"id":"FWL0A-DRP2t7","colab_type":"code","colab":{}},"source":["# Create the model with embedding\n","model = Sequential(name=\"emb_model\")\n","model.add(Embedding(input_dim=max_vocabulary, output_dim=wordvec_dim, input_length=max_len))\n","model.add(SimpleRNN(units=128))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","# Load pre-trained weights\n","model.load_weights('embedding_model_weights.h5')\n","\n","# Evaluate the models' performance (ignore the loss value)\n","_, acc = model.evaluate(X_test, y_test, verbose=0)\n","\n","# Print the results\n","print(\"SimpleRNN model's accuracy:\\t{0}\\nEmbeddings model's accuracy:\\t{1}\".format(acc_simpleRNN, acc))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7fBkratfP2ot","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}