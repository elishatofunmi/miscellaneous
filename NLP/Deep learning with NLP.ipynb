{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recurrent neural networks are used when a data\n",
    "pattern changes over time. RNNs can be assumed as \n",
    "unrolled over time.\n",
    "\n",
    "RNNs have feedback loops in which the output from the previous firing or time index T is fed as one of the inputs at time index T+1. There might be cases in which the output of the neuron is fed to itself as input. These are well suited for applications involving sequences.\n",
    "\n",
    "Encoding RNN- Enables network to take an input of the sequence form.\n",
    "\n",
    "Generating RNN- network basically outputs a sequence of numbers or values, like words in a sentence.\n",
    "\n",
    "General recurrent neural networks - combination of the preceding two types of RNNs. It's used to generate sequences and, thus, are widely used in NLG (natural language generation) tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# encoder - decoder networks\n",
    "This uses one network to create an internal representation of the input, or to 'encode' it, and that representation is used as an input for another network to produce the output.\n",
    "\n",
    "## recursive neural networks\n",
    "In a recursive neural network, a fixed set of weights is recursively applied onto the network structure and is primarily used to discover, the hierarchy or structure of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, False, False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "name = [1,2,3,4,5]\n",
    "name1 = [1,2.2,4,1,1]\n",
    "[np.round(i)==j for i,j in zip(name1, name)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word vector representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduction to word Embedding\n",
    "#### Neural Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feedforward neural net language model (FNNLM) proposed by Bengio introduces a feedforward neural network consisting of a single hidden layer than prdicts the future words. \n",
    "1. Embedding layer: This keeps a record of the representation of all the words in the training dataset. It is initialized with a set of random weights. The embedding layer is made up of three parts, which includes the size of the vocabulary, the output size of the vector in which all the words will be embedded, and the length of the input sequences to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word2vec code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, collections, math, os, zipfile, time, re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pylab\n",
    "%matplotlib inline\n",
    "\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "\n",
    "\"\"\"make sure the dataset link is copied correctly\"\"\"\n",
    "\n",
    "dataset_link = 'http://mattmahoney.net/dc/'\n",
    "zip_file = 'text8.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_download(zip_file):\n",
    "    \"\"\"downloading the required file\"\"\"\n",
    "    if not os.path.exists(zip_file):\n",
    "        zip_file, _ = urlretrieve(dataset_link+zip_file, zip_file)\n",
    "        print(\"file downloaded successfully\")\n",
    "        \n",
    "    return None\n",
    "data_download(zip_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Extracting the dataset in separate folder\"\"\"\n",
    "extracted_folder = 'dataset'\n",
    "if not os.path.isdir(extracted_folder):\n",
    "    with zipfile.ZipFile(zip_file) as zf:\n",
    "        zf.extractall(extracted_folder)\n",
    "with open('dataset/text8') as ft_:\n",
    "    full_text = ft_.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(ft8_text):\n",
    "    \"\"\"Replacing punctuation marks with tokens\"\"\"\n",
    "    ft8_text = ft8_text.lower()\n",
    "    ft8_text = ft8_text.replace('.', ' <period> ')\n",
    "    ft8_text = ft8_text.replace(',', ' <comma> ')\n",
    "    ft8_text = ft8_text.replace('\"', ' <quotation> ')\n",
    "    ft8_text = ft8_text.replace(';', ' <semicolon> ')\n",
    "    ft8_text = ft8_text.replace('!', ' <exclamation> ')\n",
    "    ft8_text = ft8_text.replace('?', ' <question> ')\n",
    "    ft8_text = ft8_text.replace('(', ' <paren_l> ')\n",
    "    ft8_text = ft8_text.replace(')', ' <paren_r> ')\n",
    "    ft8_text = ft8_text.replace('--', ' <hyphen> ')\n",
    "    ft8_text = ft8_text.replace(':', ' <colon> ')\n",
    "    ft8_text_tokens = ft8_text.split()\n",
    "    return ft8_text_tokens\n",
    "\n",
    "ft_tokens = text_processing(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Shortlisting words with frequency more than 7\"\"\"\n",
    "word_cnt = collections.Counter(ft_tokens)\n",
    "shortlisted_words = [w for w in ft_tokens if word_cnt[w] > 7 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against', 'early', 'working', 'class', 'radicals', 'including']\n"
     ]
    }
   ],
   "source": [
    "print(shortlisted_words[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The continuous bag-of-words mode l shares an architectural similarity to the FNNLM.\n",
    "\n",
    "### Subsampling frequent words\n",
    "### Negative sampling\n",
    "\n",
    "Negative sampling is a simplified form of the noise contrastive estimation (NCE) approach, as it makes certain assumptions while selecting the count of the noise, or negative, samples and their distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skip gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creating the threshold and performing the subsampling\"\"\"\n",
    "thresh = 0.00005\n",
    "word_counts = collections.Counter(word_cnt)\n",
    "total_count = len(word_cnt)\n",
    "freqs = {word: count / total_count for word, count in word_counts.items()}\n",
    "p_drop = {word: 1 - np.sqrt(thresh/freqs[word]) for word in\n",
    "word_counts}\n",
    "train_words = [word for word in word_cnt if p_drop[word] <\n",
    "random.random()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235478"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_target_set_generation(batch_, batch_index, word_window):\n",
    "    \"\"\"The function combines the words of given word_window\n",
    "size next to the index, for the SkipGram model\"\"\"\n",
    "    random_num = np.random.randint(1, word_window+1)\n",
    "    words_start = batch_index - random_num if (batch_index -\n",
    "random_num) > 0 else 0\n",
    "    words_stop = batch_index + random_num\n",
    "    window_target = set(batch_[words_start:batch_index] +\n",
    "batch_[batch_index+1:words_stop+1])\n",
    "    return list(window_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipG_batch_creation(short_words, batch_length, word_window):\n",
    "    \"\"\"The function internally makes use of the skipG_target_\n",
    "set_generation() function and combines each of the label\n",
    "    words in the shortlisted_words with the words of word_\n",
    "window size around\"\"\"\n",
    "    batch_cnt = len(short_words)//batch_length\n",
    "    short_words = short_words[:batch_cnt*batch_length]  \n",
    "    for word_index in range(0, len(short_words), batch_length):\n",
    "        input_words, label_words = [], []\n",
    "        word_batch = short_words[word_index:word_index+batch_length]\n",
    "        for index_ in range(len(word_batch)):\n",
    "            batch_input = word_batch[index_]\n",
    "            batch_label = skipG_target_set_generation(word_batch, index_, word_window)\n",
    "            # Appending the label and inputs to the initial\n",
    "#list. Replicating input to the size of labels in the window\n",
    "            label_words.extend(batch_label)\n",
    "            input_words.extend([batch_input]*len(batch_label))\n",
    "            yield input_words, label_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() \n",
    "\n",
    "tf_graph = tf.Graph()\n",
    "with tf_graph.as_default():\n",
    "    input_ = tf.placeholder(tf.int32, [None], name='input_')\n",
    "    label_ = tf.placeholder(tf.int32, [None, None],name='label_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.0'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rev_dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-2f5f774b903d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mword_embed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_uniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrev_dictionary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rev_dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "with tf_graph.as_default():\n",
    "    word_embed = tf.Variable(tf.random_uniform((len(rev_dictionary),300), -1,1))\n",
    "    embedding = tf.nn.embedding_lookup(word_embed, input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
