{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/2554/2554.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = urlopen(url).read()\n",
    "type(raw)\n",
    "len(raw)\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization\n",
    "\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "type(tokens)\n",
    "len(tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(tokens)\n",
    "type(text)\n",
    "text[1020:1060]\n",
    "text.collection()\n",
    "\n",
    "raw.find('PART I')\n",
    "raw.rfind(\"End of Project Gutenberg's Crime\")\n",
    "raw = raw[5303:1157681]\n",
    "raw.find('Part I')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = urlopen(url).read()\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = nltk.clean_html(html)\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokens[96:399]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance('gene')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 317kB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: pypdf2\n",
      "  Building wheel for pypdf2 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pypdf2: filename=PyPDF2-1.26.0-cp37-none-any.whl size=61086 sha256=effd53483bee718326ff99cbf7c31ed499c8956312085db5a1c95253309f2c5f\n",
      "  Stored in directory: /home/odemakinde/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
      "Successfully built pypdf2\n",
      "Installing collected packages: pypdf2\n",
      "Successfully installed pypdf2-1.26.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install pypdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Topic Modeling with SVD & NMF (NLP video 2).mp4',\n",
       " 'NLP.ipynb',\n",
       " 'What is NLP (NLP video 1).mp4',\n",
       " 'text corporal.ipynb',\n",
       " '.ipynb_checkpoints',\n",
       " 'Topic Modeling & SVD revisited (NLP video 3).mp4',\n",
       " 'Sentiment Classification with Naive Bayes (NLP video 4).mp4',\n",
       " 'SVD and NMF.ipynb']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "def extract_information(pdf_path):\n",
    "    with open(pdf_path, 'rb') as f:\n",
    "        pdf = PdfFileReader(f)\n",
    "        information = pdf.getDocumentInfo()\n",
    "        number_of_pages = pdf.getNumPages()\n",
    "        print('number of pages: ',number_of_pages)\n",
    "    return information, number_of_pages\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/odemakinde/Desktop/NLP'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of pages:  273\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'/Author': '',\n",
       "  '/Title': '',\n",
       "  '/Subject': '',\n",
       "  '/Creator': 'LaTeX with hyperref package',\n",
       "  '/Producer': 'pdfTeX-1.40.16',\n",
       "  '/Keywords': '',\n",
       "  '/CreationDate': 'D:20171105000353Z',\n",
       "  '/ModDate': 'D:20171105000353Z',\n",
       "  '/Trapped': '/False',\n",
       "  '/PTEX.Fullbanner': 'This is pdfTeX, Version 3.14159265-2.6-1.40.16 (TeX Live 2015/Debian) kpathsea version 6.2.1'},\n",
       " 273)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_information('/home/odemakinde/Desktop/opencv-python-tutroals-latest.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "# rotate page\n",
    "def rotate_pages(pdf_path):\n",
    "    pdf_writer = PdfFileWriter()\n",
    "    pdf_reader = PdfFileReader(pdf_path)\n",
    "    # Rotate page 90 degrees to the right\n",
    "    page_1 = pdf_reader.getPage(0).rotateClockwise(90)\n",
    "    pdf_writer.addPage(page_1)\n",
    "    # Rotate page 90 degrees to the left\n",
    "    page_2 = pdf_reader.getPage(1).rotateCounterClockwise(90)\n",
    "    pdf_writer.addPage(page_2)\n",
    "    # Add a page in normal orientation\n",
    "    pdf_writer.addPage(pdf_reader.getPage(2))\n",
    "\n",
    "    with open('rotate_pages.pdf', 'wb') as fh:\n",
    "        pdf_writer.write(fh)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = 'Jupyter_Notebook_An_Introduction.pdf'\n",
    "    #rotate_pages(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_pdfs(paths, output):\n",
    "    pdf_writer = PdfFileWriter()\n",
    "\n",
    "    for path in paths:\n",
    "        pdf_reader = PdfFileReader(path)\n",
    "        for page in range(pdf_reader.getNumPages()):\n",
    "            # Add each page to the writer object\n",
    "            pdf_writer.addPage(pdf_reader.getPage(page))\n",
    "\n",
    "    # Write out the merged PDF\n",
    "    with open(output, 'wb') as out:\n",
    "        pdf_writer.write(out)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    paths = ['document1.pdf', 'document2.pdf']\n",
    "    merge_pdfs(paths, output='merged.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_splitting.py\n",
    "\n",
    "from PyPDF2 import PdfFileReader, PdfFileWriter\n",
    "\n",
    "def split(path, name_of_split):\n",
    "    pdf = PdfFileReader(path)\n",
    "    for page in range(pdf.getNumPages()):\n",
    "        pdf_writer = PdfFileWriter()\n",
    "        pdf_writer.addPage(pdf.getPage(page))\n",
    "\n",
    "        output = f'{name_of_split}{page}.pdf'\n",
    "        with open(output, 'wb') as output_pdf:\n",
    "            pdf_writer.write(output_pdf)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    path = 'Jupyter_Notebook_An_Introduction.pdf'\n",
    "    split(path, 'jupyter_page')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #pdf_watermarker.py\n",
    "\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "\n",
    "def create_watermark(input_pdf, output, watermark):\n",
    "    watermark_obj = PdfFileReader(watermark)\n",
    "    watermark_page = watermark_obj.getPage(0)\n",
    "\n",
    "    pdf_reader = PdfFileReader(input_pdf)\n",
    "    pdf_writer = PdfFileWriter()\n",
    "\n",
    "    # Watermark all the pages\n",
    "    for page in range(pdf_reader.getNumPages()):\n",
    "        page = pdf_reader.getPage(page)\n",
    "        page.mergePage(watermark_page)\n",
    "        pdf_writer.addPage(page)\n",
    "\n",
    "    with open(output, 'wb') as out:\n",
    "        pdf_writer.write(out)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_watermark(\n",
    "        input_pdf='Jupyter_Notebook_An_Introduction.pdf', \n",
    "        output='watermarked_notebook.pdf',\n",
    "        watermark='watermark.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "\n",
    "def add_encryption(input_pdf, output_pdf, password):\n",
    "    pdf_writer = PdfFileWriter()\n",
    "    pdf_reader = PdfFileReader(input_pdf)\n",
    "\n",
    "    for page in range(pdf_reader.getNumPages()):\n",
    "        pdf_writer.addPage(pdf_reader.getPage(page))\n",
    "\n",
    "    pdf_writer.encrypt(user_pwd=password, owner_pwd=None, \n",
    "                       use_128bit=True)\n",
    "\n",
    "    with open(output_pdf, 'wb') as fh:\n",
    "        pdf_writer.write(fh)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    add_encryption(input_pdf='reportlab-sample.pdf',\n",
    "                   output_pdf='reportlab-encrypted.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"shall I compare thee to a Summer's day? Thou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet = \"shall I compare thee to a Summer's day? \\\n",
    "Thou are more lovely and more temperate:\"\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding Encoded Text from files\n",
    "\n",
    "import codecs\n",
    "f = codecs.open(path, encoding = 'lating2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# regular expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "wordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-90a76ffd756d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-90a76ffd756d>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    [w for w]\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "[w for w in wordlist if re.search('ed$', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search('^..j..t..$',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$',w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "wsj = [\"3000\",\"Amen\", \"you are here\",\"low_resolution_frame2009.jpg\",\"high_resolution_rgb2020.jpg\"]\n",
    "[w for w in wsj if re.search('^[A-Z]{3}+_[A-Z]{10}+_[A-Z]+[0-9]+\\.[A-Z]', w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"\n",
    "    DENNIS: Listen, strange women lying in ponds distributing\n",
    "    swords is no basis for a system of government. Surpreme\n",
    "    executive power derive from a mandate form the masses, not \n",
    "    some farcical aquatic ceremony.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemmers\n",
    "from nltk import *\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)\n",
    "[porter.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexedText:\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i) for (i,word) in enumerate(text))\n",
    "                                 \n",
    "    def concordance(self, word, width =40):\n",
    "        key = self._stem(word)\n",
    "        wc = width/4 #words of context\n",
    "        \n",
    "        for i in self._index[key]:\n",
    "            lcontext = ' '.join(self._text[i-wc:i])\n",
    "            rcontext = ' '.join(self._text[i:i+wc])\n",
    "            ldisplay = '%*s' % (width, lcontext[-width:])\n",
    "            rdisplay = '%-*s' % (width, rcontext[:width])\n",
    "            print(ldisplay, rdisplay)\n",
    "            \n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words('grail.txt')\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance('lie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'networkx' has no attribute 'draw_graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-25142bc5931c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mdog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dog.n.01'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyponym_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mgraph_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-25142bc5931c>\u001b[0m in \u001b[0;36mgraph_draw\u001b[0;34m(graph)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgraph_draw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     nx.draw_graphviz(graph,\n\u001b[0m\u001b[1;32m     15\u001b[0m                    \u001b[0mnode_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m16\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdegree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                    \u001b[0mnode_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'networkx' has no attribute 'draw_graphviz'"
     ]
    }
   ],
   "source": [
    "def traverse(graph, start, node):\n",
    "    graph.depth[node.name] = node.shortest_path_distance(start)\n",
    "    for child in node.hyponyms():\n",
    "        graph.add_edge(node.name, child.name)\n",
    "        traverse(graph, start ,child)\n",
    "        \n",
    "def hyponym_graph(start):\n",
    "    G = nx.Graph()\n",
    "    G.depth = {}\n",
    "    traverse(G, start, start)\n",
    "    return G\n",
    "\n",
    "def graph_draw(graph):\n",
    "    nx.draw_graphviz(graph,\n",
    "                   node_size = [16 * graph.degree(n) for n in graph],\n",
    "                   node_color = [graph.depth[n] for n in graph],\n",
    "                   with_labels = False)\n",
    "    matplotlib.pyplot.show()\n",
    "    \n",
    "    \n",
    "dog = wn.synset('dog.n.01')\n",
    "graph = hyponym_graph(dog)\n",
    "graph_draw(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# categorizing and tagging words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of classifying words into their parts-of-speech and labeling them accordingly is known as part of speech\n",
    "tagging, POS tagging, or simply tagging. Parts of speech are also known as word classes or lexical categories.\n",
    "The collection of tags used for a particular task is known as a tagset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/odemakinde/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = nltk.word_tokenize(\"and now for something completely different\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/odemakinde/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('and', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('for', 'IN'),\n",
       " ('something', 'NN'),\n",
       " ('completely', 'RB'),\n",
       " ('different', 'JJ')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/odemakinde/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: the fulton county grand jury said friday an...>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "man time day year car moment world house family child country boy\n",
      "state job place way war girl work word\n"
     ]
    }
   ],
   "source": [
    "text.similar('woman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time day and one it way year woman state house men world life car\n",
      "people war church that place work\n"
     ]
    }
   ],
   "source": [
    "text.similar('man')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "made said done put had seen found given left heard was been brought\n",
      "set got that took in told felt\n"
     ]
    }
   ],
   "source": [
    "text.similar('bought')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
