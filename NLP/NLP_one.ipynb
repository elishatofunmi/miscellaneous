{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'John has been selected for the trial phase this time. Congrats!!'\n",
    "\n",
    "sentence = sentence.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'john has been selected for the trial phase this time. congrats'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_words = ['awesome', 'good', 'nice', 'super','fun',\n",
    "                 'delightful','congrats']\n",
    "negative_words = ['awful','lame','horrible','bad']\n",
    "sentence = sentence.replace('!','')\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['john',\n",
       " 'has',\n",
       " 'been',\n",
       " 'selected',\n",
       " 'for',\n",
       " 'the',\n",
       " 'trial',\n",
       " 'phase',\n",
       " 'this',\n",
       " 'time.',\n",
       " 'congrats']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = sentence.split(' ')\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'been',\n",
       " 'for',\n",
       " 'has',\n",
       " 'john',\n",
       " 'phase',\n",
       " 'selected',\n",
       " 'the',\n",
       " 'this',\n",
       " 'time.',\n",
       " 'trial'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = set(words) - set(positive_words)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'been',\n",
       " 'congrats',\n",
       " 'for',\n",
       " 'has',\n",
       " 'john',\n",
       " 'phase',\n",
       " 'selected',\n",
       " 'the',\n",
       " 'this',\n",
       " 'time.',\n",
       " 'trial'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_two = set(words) - set(negative_words)\n",
    "result_two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/odemakinde/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning)\n"
     ]
    }
   ],
   "source": [
    "# accessing text from the web\n",
    "\n",
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "pool_object = urllib3.PoolManager()\n",
    "target_url= 'https://www.gutenberg.org/files/2554/2554-h/2554-h.htm#link2HCH0008'\n",
    "response = pool_object.request('GET', target_url)\n",
    "final_html_txt= BeautifulSoup(response.data)\n",
    "#print(final_html_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Crime and Punishment, by Fyodor Dostoevsky', 'The Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r\\n\\r\\nThis eBook is for the use of anyone anywhere at no cost and with\\r\\nalmost no restrictions whatsoever.  You may copy it, give it away or\\r\\nre-use it under the terms of the Project Gutenberg License included\\r\\nwith this eBook or online at www.gutenberg.org\\r\\n\\r\\n\\r\\nTitle: Crime and Punishment\\r\\n\\r\\nAuthor: Fyodor Dostoevsky\\r\\n\\r\\nRelease Date: March 28, 2006 [EBook #2554]\\r\\nLast Updated: October 27, 2016\\r\\n\\r\\nLanguage: English\\r\\n\\r\\nCharacter set encoding: UTF-8\\r\\n\\r\\n*** START OF THIS PROJECT GUTENBERG EBOOK CRIME AND PUNISHMENT ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced by John Bickers; and Dagny and David Widger', 'CRIME AND PUNISHMENT', 'By Fyodor Dostoevsky', 'Translated By Constance Garnett']\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "html = urlopen(target_url).read()\n",
    "\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "\n",
    "for script in soup([\"script\", \"style\"]):\n",
    "\n",
    "    script.decompose()\n",
    "\n",
    "strips = list(soup.stripped_strings)\n",
    "\n",
    "print(strips[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4523"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(strips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'processing',\n",
       " '!']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "sentence = \"This book is about Deep Learning and Natural Language processing!\"\n",
    "tokens = word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/odemakinde/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stopword is a commonly used word (such as the)) that a \n",
    "#search engine has been programmed to ignore\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['as', 't', 'do', 'ma', 'her', 'whom', \"wasn't\", 'now', \"haven't\", \"couldn't\"]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(stop_words)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'book', 'Deep', 'Learning', 'Natural', 'Language', 'processing', '!']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokens = [w for w in tokens if not w in stop_words]\n",
    "new_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'book',\n",
       " 'is',\n",
       " 'about',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " 'and',\n",
       " 'Natural',\n",
       " 'Language',\n",
       " 'processing',\n",
       " '!']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# count vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a scikit-learn library tool that takes any mass of text and returns each unique word as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['also', 'and', 'classic', 'he', 'listens', 'music', 'old', 'pop', 'ramiess', 'rock', 'sings', 'songs', 'to']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "texts = ['Ramiess sings classic songs', 'he listens to old pop',\n",
    "        'and rock music','and also listens to classic songs']\n",
    "cv = CountVectorizer()\n",
    "cv_fit= cv.fit_transform(texts)\n",
    "print(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 0 0 1 1 0 1 1 0 0 0 0 1]\n",
      " [0 1 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [1 1 1 0 1 0 0 0 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(cv_fit.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_fit.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Score\n",
    "TF-IDF is an acronymn of two terms: term frequency and inverse document frequency. TF is the ratio representing the count of specific words to the total number of words in a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.52547275 0.         0.         0.\n",
      "  0.         0.         0.         0.52547275 0.         0.52547275\n",
      "  0.41428875 0.        ]\n",
      " [0.         0.         0.         0.         0.48546061 0.38274272\n",
      "  0.         0.48546061 0.48546061 0.         0.         0.\n",
      "  0.         0.38274272]\n",
      " [0.         0.48693426 0.         0.         0.         0.\n",
      "  0.61761437 0.         0.         0.         0.61761437 0.\n",
      "  0.         0.        ]\n",
      " [0.47212003 0.37222485 0.         0.47212003 0.         0.37222485\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.37222485 0.37222485]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "texts= ['Ramiess sings classic songs', 'he listens to old pop',\n",
    "       'and rock music','and also listens to classical songs']\n",
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(texts)\n",
    "print(X.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 14)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f356bc088d0>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/textblob/\u001b[0m\n",
      "\u001b[33m  WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f356bc78f10>: Failed to establish a new connection: [Errno -2] Name or service not known')': /simple/textblob/\u001b[0m\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/f0/1d9bfcc8ee6b83472ec571406bd0dd51c0e6330ff1a51b2d29861d389e85/textblob-0.15.3-py2.py3-none-any.whl (636kB)\n",
      "\u001b[K     |████████████████████████████████| 645kB 379kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/odemakinde/anaconda3/lib/python3.7/site-packages (from textblob) (3.4.5)\n",
      "Requirement already satisfied: six in /home/odemakinde/anaconda3/lib/python3.7/site-packages (from nltk>=3.1->textblob) (1.12.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.15.3\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Classifier\n",
    "\n",
    "from textblob import TextBlob\n",
    "from textblob.classifiers import NaiveBayesClassifier\n",
    "data = [\n",
    "    ('I love my country','pos'),\n",
    "    ('This is an amazing place!', 'pos'),\n",
    "    ('I do not like the smell of this place.', 'neg'),\n",
    "    ('I do not like this restaurant','neg'),\n",
    "    ('I am tired of hearing your nonsense.','neg'),\n",
    "    ('I always aspire to be like him', 'pos'),\n",
    "    (\"it's a horrible performance.\", 'neg')\n",
    "]\n",
    "\n",
    "model = NaiveBayesClassifier(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(\"It's an awesome place!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(\"i am here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(\"you are not serious\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classify(\"I do love you\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
