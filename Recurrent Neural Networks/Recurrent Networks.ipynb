{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "# Basic RNNs in Tensorflow\n",
    "import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "x0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "x1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "wx = tf.Variable(tf.random_normal(shape = [n_inputs, n_neurons], dtype = tf.float32))\n",
    "wy = tf.Variable(tf.random_normal(shape = [n_neurons, n_neurons], dtype = tf.float32))\n",
    "b = tf.Variable(tf.zeros([1, n_neurons], dtype = tf.float32))\n",
    "\n",
    "y0 = tf.tanh(tf.matmul(x0, wx) +b)\n",
    "y1 = tf.tanh(tf.matmul(y0, wy)+ tf.matmul(x1, wx) +b)\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#Mini-batch \n",
    "x0_batch = np.array([[0,1,2],[3,4,5],[6,7,8],[9,0,1]]) #t = 0\n",
    "x1_batch = np.array([[9,8,7],[0,0,0],[6,5,4],[3,2,1]]) #t = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    y0_val, y1_val = sess.run([y0, y1], feed_dict = {x0:x0_batch, x1:x1_batch})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9293567  -0.8781928  -0.96271807 -0.39214566 -0.8828403 ]\n",
      " [-0.9978528   0.8130378  -0.99999803  0.81579167 -0.9916774 ]\n",
      " [-0.9999369   0.99862164 -1.          0.9910529  -0.99943894]\n",
      " [-0.9551768   1.         -0.6346127   1.          0.9995964 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y0_val) #output at t = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9999985   0.9999997  -1.          0.99999964 -0.9293559 ]\n",
      " [-0.968734   -0.99417424  0.4295082   0.73013866 -0.6252687 ]\n",
      " [-0.9989001   0.9969851  -0.99999934  0.99957544 -0.97793955]\n",
      " [ 0.94890285 -0.4627683   0.41344348  0.9921769  -0.4482873 ]]\n"
     ]
    }
   ],
   "source": [
    "print(y1_val) #output at t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-6-8a35342e1f2d>:6: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-6-8a35342e1f2d>:7: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:456: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n",
      "WARNING:tensorflow:From /home/odemakinde/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/rnn_cell_impl.py:460: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "# Building the same model using Tensorflow's RNN operations.\n",
    "x0 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "x1 = tf.placeholder(tf.float32, [None, n_inputs])\n",
    "\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons)\n",
    "output_seqs, state = tf.nn.static_rnn(basic_cell, [x0, x1], dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y0, y1 = output_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-write (better)\n",
    "n_steps = 2\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "X_seqs = tf.unstack(tf.transpose(X, perm = [1,0,2]))\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons, reuse = True)\n",
    "output_seqs, state = tf.nn.static_rnn(basic_cell, X_seqs, dtype = tf.float32)\n",
    "outputs = tf.transpose(tf.stack(output_seqs), perm = [1,0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2, 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_batch = np.array([\n",
    "    #t = 0      t =1\n",
    "    [[0,1,2],[9,8,7]],\n",
    "    [[3,4,5],[0,0,0]],\n",
    "    [[6,7,8],[6,5,4]],\n",
    "    [[9,8,1],[3,2,1]],\n",
    "])\n",
    "x_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with tf.Session() as sess:\n",
    "    pass\n",
    "    #init.run()\n",
    "    #output_val = outputs.eval(feed_dict = {X:x_batch})\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach still builds a graph containing one cell per time step. If there were 50 time steps, the graph would look pretty ugly. With such a large graph, you may even get out-of-memory (OOM) errors during backpropagation (especially with the limited memory of GPU cards), since it must store all tensor values during the forward pass so it can use them to compute gradient during the reverse pass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dynamic unrolling through time\n",
    "\n",
    "The dynamic_rnn() function uses a while loop operation to run over the ell the appropriate number of times, and you can set swap_memory = True if you want it to swap the GPU's memory to the CPU's memory during backpropagation to avoid OOM errors.\n",
    "\n",
    "Conviniently, it also accepts a single tensor for all inputs at every time step (shape [None, n_steps, n_inputs]) and it outputs a single tensor for all outputs at every time step (shape [none, n_steps, n_neurons]), there is no need to stack, unstack or transpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-26-20c23ee12ed8>:4: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n"
     ]
    }
   ],
   "source": [
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons, reuse = True)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype = tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so far we have used only-fixed size input sequences (all exactly two steps long). what if the input sequences have variable lengths, (e.g., like sentences)? In this case you should set the sequence_length parameter when calling the dynamic_rnn() (or static_rnn()) function; it must be a 1D tensor indicating the length of the input sequence for each instance. For example, \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.nn.rnn_cell.BasicRNNCell(num_units = n_neurons, reuse = True)\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X,\n",
    "                                    dtype = tf.float32,\n",
    "                                   sequence_length = seq_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for example, suppose the second input sequence contains only one input instead of two. It must be padded with zero vector in order to fit in the input tensor X (because the input tensor's second dimension is the size of the longest sequence--i.e., 2).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length_batch = np.array([2,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    #outputs_val, states_val = sess.run([outputs, states],\n",
    "    #                                  feed_dict = {X:x_batch, seq_length:seq_length_batch})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To handle variable-length output sequences\n",
    "\n",
    "In this case, most common solution is to define a special output called an end-of-sequence token (EOS token). Any output past the EOS should be ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
