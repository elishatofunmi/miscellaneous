{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from docx import Document\n",
    "\n",
    "def read_docx_tab(tab, **kwargs):\n",
    "    vf = io.StringIO()\n",
    "    writer = csv.writer(vf)\n",
    "    for row in tab.rows:\n",
    "        writer.writerow(cell.text for cell in row.cells)\n",
    "    vf.seek(0)\n",
    "    return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "def read_docx_tables(filename, tab_id=None, **kwargs):\n",
    "    doc = Document(filename)\n",
    "    if tab_id is None:\n",
    "        return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "    else:\n",
    "        try:\n",
    "            return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "        except IndexError:\n",
    "            print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "            raise\n",
    "\n",
    "            \n",
    "table = read_docx_tables(filename = 'Tabular.docx', tab_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S/N</th>\n",
       "      <th>ENG VERB</th>\n",
       "      <th>ENG VERB.1</th>\n",
       "      <th>PAST TENSE</th>\n",
       "      <th>PAST TENSE .1</th>\n",
       "      <th>PAST TENSE .2</th>\n",
       "      <th>English Sentence</th>\n",
       "      <th>IGBO SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>I found the book</td>\n",
       "      <td>Áchọ̀tárà ḿ ákwúkwọ́</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>DO</td>\n",
       "      <td>DO</td>\n",
       "      <td>DID</td>\n",
       "      <td>DID</td>\n",
       "      <td>DID</td>\n",
       "      <td>I did it</td>\n",
       "      <td>émèrè ḿ yá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>He made  the cake</td>\n",
       "      <td>émèrè ḿ áchíchá ahù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GET</td>\n",
       "      <td>GET</td>\n",
       "      <td>GOT</td>\n",
       "      <td>GOT</td>\n",
       "      <td>GOT</td>\n",
       "      <td>I got home</td>\n",
       "      <td>énwò ḿ n'ụ́lọ̀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAID</td>\n",
       "      <td>SAID</td>\n",
       "      <td>SAID</td>\n",
       "      <td>He said nothing</td>\n",
       "      <td>O kwụ́ghi ihé ọ bụ́lá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S/N ENG VERB ENG VERB.1 PAST TENSE  PAST TENSE .1 PAST TENSE .2  \\\n",
       "0    1     FIND       FIND      FOUND         FOUND         FOUND    \n",
       "1    2      DO         DO         DID           DID           DID    \n",
       "2    3    MAKE       MAKE         MADE          MADE          MADE   \n",
       "3    4      GET        GET        GOT           GOT           GOT    \n",
       "4    5      SAY        SAY      SAID          SAID          SAID     \n",
       "\n",
       "   English Sentence         IGBO SENTENCES   \n",
       "0   I found the book   Áchọ̀tárà ḿ ákwúkwọ́  \n",
       "1          I did it              émèrè ḿ yá  \n",
       "2  He made  the cake    émèrè ḿ áchíchá ahù  \n",
       "3         I got home         énwò ḿ n'ụ́lọ̀  \n",
       "4   He said nothing   O kwụ́ghi ihé ọ bụ́lá  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S/N', 'ENG VERB', 'ENG VERB.1', 'PAST TENSE ', 'PAST TENSE .1',\n",
       "       'PAST TENSE .2', 'English Sentence ', 'IGBO SENTENCES '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_table = table.drop(['S/N','ENG VERB.1','PAST TENSE .1','PAST TENSE .2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENG VERB</th>\n",
       "      <th>PAST TENSE</th>\n",
       "      <th>English Sentence</th>\n",
       "      <th>IGBO SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>I found the book</td>\n",
       "      <td>Áchọ̀tárà ḿ ákwúkwọ́</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>DO</td>\n",
       "      <td>DID</td>\n",
       "      <td>I did it</td>\n",
       "      <td>émèrè ḿ yá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>He made  the cake</td>\n",
       "      <td>émèrè ḿ áchíchá ahù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GET</td>\n",
       "      <td>GOT</td>\n",
       "      <td>I got home</td>\n",
       "      <td>énwò ḿ n'ụ́lọ̀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAID</td>\n",
       "      <td>He said nothing</td>\n",
       "      <td>O kwụ́ghi ihé ọ bụ́lá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ENG VERB PAST TENSE   English Sentence         IGBO SENTENCES \n",
       "0     FIND      FOUND    I found the book   Áchọ̀tárà ḿ ákwúkwọ́\n",
       "1      DO         DID           I did it              émèrè ḿ yá\n",
       "2    MAKE         MADE  He made  the cake    émèrè ḿ áchíchá ahù\n",
       "3      GET        GOT          I got home         énwò ḿ n'ụ́lọ̀\n",
       "4      SAY      SAID     He said nothing   O kwụ́ghi ihé ọ bụ́lá"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_table = updated_table.dropna()\n",
    "updated_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = updated_table['English Sentence '], updated_table['IGBO SENTENCES ']\n",
    "raw_dataset = updated_table[['English Sentence ','IGBO SENTENCES ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I found the book\n",
       "1           I did it \n",
       "Name: English Sentence , dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all letters\n",
    "x_lower = x.apply(lambda x:x.lower())\n",
    "y_lower = y.apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lower[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes\n",
    "import re\n",
    "x_lower = x_lower.apply(lambda x:re.sub(\"'\",'',x))\n",
    "y_lower = y_lower.apply(lambda x:re.sub(\"'\",'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation) # set of all special characters\n",
    "#remove all the special characters\n",
    "x_lower = x_lower.apply(lambda x:''.join(ch for ch in x if ch not in exclude))\n",
    "y_lower = y_lower.apply(lambda x:''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lower[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all numbers from text\n",
    "digits = string.digits\n",
    "remove_digits = str.maketrans('','',digits)\n",
    "x_lower = x_lower.apply(lambda x:x.translate(remove_digits))\n",
    "y_lower = y_lower.apply(lambda x:x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra spaces\n",
    "x_lower = x_lower.apply(lambda x: x.strip())\n",
    "y_lower = y_lower.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end tokens to target sequences\n",
    "y_lower = y_lower.apply(lambda x: 'START_ '+x +' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get english and igbo vocabulary\n",
    "all_eng_words = set()\n",
    "for eng in x_lower:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "            \n",
    "            \n",
    "all_igbo_words = set()\n",
    "\n",
    "for igb in y_lower:\n",
    "    for word in igb.split():\n",
    "        if word not in all_igbo_words:\n",
    "            all_igbo_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "print(len(all_eng_words))\n",
    "print(len(all_igbo_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 183)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_igbo_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_igbo_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i,word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i,word) for word, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     i found the book\n",
       "1             i did it\n",
       "2    he made  the cake\n",
       "3           i got home\n",
       "Name: English Sentence , dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lower[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74,), (19,))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_lower, y_lower, test_size = 0.2)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55    he promoted the argument\n",
       "49      she developed the idea\n",
       "Name: English Sentence , dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = 5\n",
    "max_decoder_seq_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(x_train), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(x_train), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(x_train), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_me = ['i', ' ', 'am', 'her']\n",
    "out = list_me.remove(' ')\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(x_train, y_train)):\n",
    "    x_transform = input_text.split(' ')\n",
    "    try:\n",
    "        x_transform.remove(' ')\n",
    "    except ValueError as err:\n",
    "        continue\n",
    "        \n",
    "    for t, char in enumerate(x_transform):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t + 1:, input_token_index[' ']] = 1.\n",
    "    y_transform = target_text.split(' ')\n",
    "    try:\n",
    "        y_transform.remove(' ')\n",
    "    except ValueError as err:\n",
    "        continue\n",
    "    \n",
    "    for t, char in enumerate(y_transform):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n",
    "    decoder_input_data[i, t + 1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Input\n",
    "from keras.layers import LSTM, Dense\n",
    "latent_dim = 8\n",
    "\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 59 samples, validate on 15 samples\n",
      "Epoch 1/10\n",
      "59/59 [==============================] - 1s 16ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 2/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 3/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "59/59 [==============================] - 0s 2ms/step - loss: 0.0000e+00 - accuracy: 0.9831 - val_loss: 0.0000e+00 - val_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fd82e39cb90>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "epochs = 10\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save('MT.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to\n",
    "# something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index['START_']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: i found the book\n",
      "Actual sentence Translation:  START_ áchọ̀tárà ḿ ákwúkwọ́ _END\n",
      "Decoded sentence: nkirinaebu\n",
      "-\n",
      "Input sentence: i did it\n",
      "Actual sentence Translation:  START_ émèrè ḿ yá _END\n",
      "Decoded sentence: naarahụịkwụsị\n",
      "-\n",
      "Input sentence: he made  the cake\n",
      "Actual sentence Translation:  START_ émèrè ḿ áchíchá ahù _END\n",
      "Decoded sentence: naarahụịkwụsị\n",
      "-\n",
      "Input sentence: i got home\n",
      "Actual sentence Translation:  START_ énwò ḿ nụ́lọ̀ _END\n",
      "Decoded sentence: naarahụịkwụsị\n",
      "-\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-206897de0a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdecoded_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Input sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Actual sentence Translation: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mseq_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Decoded sentence:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded_sentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1066\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4728\u001b[0m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_scalar_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4729\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4730\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4731\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4732\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 4"
     ]
    }
   ],
   "source": [
    "\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', x_train[seq_index])\n",
    "    print('Actual sentence Translation: ', y_train[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
