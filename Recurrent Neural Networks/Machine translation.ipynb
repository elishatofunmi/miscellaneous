{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import csv\n",
    "from docx import Document\n",
    "\n",
    "def read_docx_tab(tab, **kwargs):\n",
    "    vf = io.StringIO()\n",
    "    writer = csv.writer(vf)\n",
    "    for row in tab.rows:\n",
    "        writer.writerow(cell.text for cell in row.cells)\n",
    "    vf.seek(0)\n",
    "    return pd.read_csv(vf, **kwargs)\n",
    "\n",
    "def read_docx_tables(filename, tab_id=None, **kwargs):\n",
    "    doc = Document(filename)\n",
    "    if tab_id is None:\n",
    "        return [read_docx_tab(tab, **kwargs) for tab in doc.tables]\n",
    "    else:\n",
    "        try:\n",
    "            return read_docx_tab(doc.tables[tab_id], **kwargs)\n",
    "        except IndexError:\n",
    "            print('Error: specified [tab_id]: {}  does not exist.'.format(tab_id))\n",
    "            raise\n",
    "\n",
    "            \n",
    "table = read_docx_tables(filename = 'Tabular.docx', tab_id = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S/N</th>\n",
       "      <th>ENG VERB</th>\n",
       "      <th>ENG VERB.1</th>\n",
       "      <th>PAST TENSE</th>\n",
       "      <th>PAST TENSE .1</th>\n",
       "      <th>PAST TENSE .2</th>\n",
       "      <th>English Sentence</th>\n",
       "      <th>IGBO SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>I found the book</td>\n",
       "      <td>Áchọ̀tárà ḿ ákwúkwọ́</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>DO</td>\n",
       "      <td>DO</td>\n",
       "      <td>DID</td>\n",
       "      <td>DID</td>\n",
       "      <td>DID</td>\n",
       "      <td>I did it</td>\n",
       "      <td>émèrè ḿ yá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>He made  the cake</td>\n",
       "      <td>émèrè ḿ áchíchá ahù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>GET</td>\n",
       "      <td>GET</td>\n",
       "      <td>GOT</td>\n",
       "      <td>GOT</td>\n",
       "      <td>GOT</td>\n",
       "      <td>I got home</td>\n",
       "      <td>énwò ḿ n'ụ́lọ̀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAID</td>\n",
       "      <td>SAID</td>\n",
       "      <td>SAID</td>\n",
       "      <td>He said nothing</td>\n",
       "      <td>O kwụ́ghi ihé ọ bụ́lá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   S/N ENG VERB ENG VERB.1 PAST TENSE  PAST TENSE .1 PAST TENSE .2  \\\n",
       "0    1     FIND       FIND      FOUND         FOUND         FOUND    \n",
       "1    2      DO         DO         DID           DID           DID    \n",
       "2    3    MAKE       MAKE         MADE          MADE          MADE   \n",
       "3    4      GET        GET        GOT           GOT           GOT    \n",
       "4    5      SAY        SAY      SAID          SAID          SAID     \n",
       "\n",
       "   English Sentence         IGBO SENTENCES   \n",
       "0   I found the book   Áchọ̀tárà ḿ ákwúkwọ́  \n",
       "1          I did it              émèrè ḿ yá  \n",
       "2  He made  the cake    émèrè ḿ áchíchá ahù  \n",
       "3         I got home         énwò ḿ n'ụ́lọ̀  \n",
       "4   He said nothing   O kwụ́ghi ihé ọ bụ́lá  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['S/N', 'ENG VERB', 'ENG VERB.1', 'PAST TENSE ', 'PAST TENSE .1',\n",
       "       'PAST TENSE .2', 'English Sentence ', 'IGBO SENTENCES '],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_table = table.drop(['S/N','ENG VERB.1','PAST TENSE .1','PAST TENSE .2'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ENG VERB</th>\n",
       "      <th>PAST TENSE</th>\n",
       "      <th>English Sentence</th>\n",
       "      <th>IGBO SENTENCES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>FIND</td>\n",
       "      <td>FOUND</td>\n",
       "      <td>I found the book</td>\n",
       "      <td>Áchọ̀tárà ḿ ákwúkwọ́</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>DO</td>\n",
       "      <td>DID</td>\n",
       "      <td>I did it</td>\n",
       "      <td>émèrè ḿ yá</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>MAKE</td>\n",
       "      <td>MADE</td>\n",
       "      <td>He made  the cake</td>\n",
       "      <td>émèrè ḿ áchíchá ahù</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>GET</td>\n",
       "      <td>GOT</td>\n",
       "      <td>I got home</td>\n",
       "      <td>énwò ḿ n'ụ́lọ̀</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>SAY</td>\n",
       "      <td>SAID</td>\n",
       "      <td>He said nothing</td>\n",
       "      <td>O kwụ́ghi ihé ọ bụ́lá</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ENG VERB PAST TENSE   English Sentence         IGBO SENTENCES \n",
       "0     FIND      FOUND    I found the book   Áchọ̀tárà ḿ ákwúkwọ́\n",
       "1      DO         DID           I did it              émèrè ḿ yá\n",
       "2    MAKE         MADE  He made  the cake    émèrè ḿ áchíchá ahù\n",
       "3      GET        GOT          I got home         énwò ḿ n'ụ́lọ̀\n",
       "4      SAY      SAID     He said nothing   O kwụ́ghi ihé ọ bụ́lá"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_table = updated_table.dropna()\n",
    "updated_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = updated_table['English Sentence '], updated_table['IGBO SENTENCES ']\n",
    "raw_dataset = updated_table[['English Sentence ','IGBO SENTENCES ']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    I found the book\n",
       "1           I did it \n",
       "Name: English Sentence , dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lowercase all letters\n",
    "x_lower = x.apply(lambda x:x.lower())\n",
    "y_lower = y.apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lower[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove quotes\n",
    "import re\n",
    "x_lower = x_lower.apply(lambda x:re.sub(\"'\",'',x))\n",
    "y_lower = y_lower.apply(lambda x:re.sub(\"'\",'',x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "exclude = set(string.punctuation) # set of all special characters\n",
    "#remove all the special characters\n",
    "x_lower = x_lower.apply(lambda x:''.join(ch for ch in x if ch not in exclude))\n",
    "y_lower = y_lower.apply(lambda x:''.join(ch for ch in x if ch not in exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    áchọ̀tárà ḿ ákwúkwọ́\n",
       "1              émèrè ḿ yá\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_lower[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all numbers from text\n",
    "digits = string.digits\n",
    "remove_digits = str.maketrans('','',digits)\n",
    "x_lower = x_lower.apply(lambda x:x.translate(remove_digits))\n",
    "y_lower = y_lower.apply(lambda x:x.translate(remove_digits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove extra spaces\n",
    "x_lower = x_lower.apply(lambda x: x.strip())\n",
    "y_lower = y_lower.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add start and end tokens to target sequences\n",
    "y_lower = y_lower.apply(lambda x: 'START_ '+x +' _END')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get english and igbo vocabulary\n",
    "all_eng_words = set()\n",
    "for eng in x_lower:\n",
    "    for word in eng.split():\n",
    "        if word not in all_eng_words:\n",
    "            all_eng_words.add(word)\n",
    "            \n",
    "            \n",
    "all_igbo_words = set()\n",
    "\n",
    "for igb in y_lower:\n",
    "    for word in igb.split():\n",
    "        if word not in all_igbo_words:\n",
    "            all_igbo_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "print(len(all_eng_words))\n",
    "print(len(all_igbo_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(177, 183)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_words = sorted(list(all_eng_words))\n",
    "target_words = sorted(list(all_igbo_words))\n",
    "num_encoder_tokens = len(all_eng_words)\n",
    "num_decoder_tokens = len(all_igbo_words)\n",
    "num_encoder_tokens, num_decoder_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(word, i+1) for i, word in enumerate(input_words)])\n",
    "target_token_index = dict([(word, i+1) for i, word in enumerate(target_words)])\n",
    "\n",
    "reverse_input_char_index = dict((i,word) for word, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i,word) for word, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'START_': 1,\n",
       " '_END': 2,\n",
       " 'abalị': 3,\n",
       " 'agaghị': 4,\n",
       " 'ahù': 5,\n",
       " 'ahịhịa': 6,\n",
       " 'ahụ': 7,\n",
       " 'akwụ̀siri': 8,\n",
       " 'akụkọ': 9,\n",
       " 'ala': 10,\n",
       " 'ama': 11,\n",
       " 'amụ̀tàrà': 12,\n",
       " 'anya': 13,\n",
       " 'anyi': 14,\n",
       " 'anyí': 15,\n",
       " 'anyị': 16,\n",
       " 'anụrụ': 17,\n",
       " 'bgàkwúnye': 18,\n",
       " 'bụla': 19,\n",
       " 'bụ́lá': 20,\n",
       " 'chọ̀rọ̀': 21,\n",
       " 'dì': 22,\n",
       " 'dịghị': 23,\n",
       " 'dọtara': 24,\n",
       " 'dọwara': 25,\n",
       " 'echere': 26,\n",
       " 'echiche': 27,\n",
       " 'ege': 28,\n",
       " 'ego': 29,\n",
       " 'egosi': 30,\n",
       " 'egwuregwu': 31,\n",
       " 'ehi': 32,\n",
       " 'enwere': 33,\n",
       " 'enye': 34,\n",
       " 'esemokwu': 35,\n",
       " 'furu': 36,\n",
       " 'gara': 37,\n",
       " 'gbanwere': 38,\n",
       " 'gbasara': 39,\n",
       " 'gburu': 40,\n",
       " 'gbàpụ̀rụ̀': 41,\n",
       " 'ghọrọ': 42,\n",
       " 'gwara': 43,\n",
       " 'gwụchara': 44,\n",
       " 'gị': 45,\n",
       " 'ha': 46,\n",
       " 'hàpụ̀rụ̀': 47,\n",
       " 'há': 48,\n",
       " 'họọrọ': 49,\n",
       " 'i': 50,\n",
       " 'ihe': 51,\n",
       " 'ihé': 52,\n",
       " 'ima': 53,\n",
       " 'ire': 54,\n",
       " 'ji': 55,\n",
       " 'jìdèrè': 56,\n",
       " 'jụ̀rụ̀': 57,\n",
       " 'kacha': 58,\n",
       " 'kpebiri': 59,\n",
       " 'kpochapuru': 60,\n",
       " 'kpuchiri': 61,\n",
       " 'kpọ̀chìrí': 62,\n",
       " 'kpọ̀rọ̀': 63,\n",
       " 'kwadoro': 64,\n",
       " 'kwalitere': 65,\n",
       " 'kwuru': 66,\n",
       " 'kwàgàrà': 67,\n",
       " 'kwèrè': 68,\n",
       " 'kwụ́ghi': 69,\n",
       " 'kèrè': 70,\n",
       " 'kụrụ': 71,\n",
       " 'laá': 72,\n",
       " 'lere': 73,\n",
       " 'm': 74,\n",
       " 'mechiri': 75,\n",
       " 'mee': 76,\n",
       " 'mepere': 77,\n",
       " 'mepụtara': 78,\n",
       " 'mere': 79,\n",
       " 'mkparịta': 80,\n",
       " 'mma': 81,\n",
       " 'màkà': 82,\n",
       " 'màlítere': 83,\n",
       " 'naachị': 84,\n",
       " 'naarahụ': 85,\n",
       " 'naebu': 86,\n",
       " 'naegwu': 87,\n",
       " 'naekiri': 88,\n",
       " 'naekwu': 89,\n",
       " 'naemeghe': 90,\n",
       " 'naeto': 91,\n",
       " 'naezo': 92,\n",
       " 'necheghị': 93,\n",
       " 'niile': 94,\n",
       " 'njikọ': 95,\n",
       " 'nke': 96,\n",
       " 'nkiri': 97,\n",
       " 'nogè': 98,\n",
       " 'notu': 99,\n",
       " 'nri': 100,\n",
       " 'ntabi': 101,\n",
       " 'ntị': 102,\n",
       " 'nubi': 103,\n",
       " 'nwara': 104,\n",
       " 'nwetara': 105,\n",
       " 'nwoke': 106,\n",
       " 'nwóke': 107,\n",
       " 'nyeèrè': 108,\n",
       " 'nyèrè': 109,\n",
       " 'nọgidere': 110,\n",
       " 'nọnú': 111,\n",
       " 'nụtụtụ': 112,\n",
       " 'nụ́lọ̀': 113,\n",
       " 'o': 114,\n",
       " 'oge': 115,\n",
       " 'ohi': 116,\n",
       " 'okwu': 117,\n",
       " 'onye': 118,\n",
       " 'osikapa': 119,\n",
       " 'osobo': 120,\n",
       " 'out': 121,\n",
       " 'ozí': 122,\n",
       " 'pụtara': 123,\n",
       " 'rụ̀sìrì': 124,\n",
       " 'sonyeere': 125,\n",
       " 'sòrò': 126,\n",
       " 'tinyèrè': 127,\n",
       " 'tụlere': 128,\n",
       " 'tụ̀ghàrị̀rị̀': 129,\n",
       " 'utịp': 130,\n",
       " 'uwe': 131,\n",
       " 'wutèrè': 132,\n",
       " 'wèré': 133,\n",
       " 'wètàrà': 134,\n",
       " 'wụ̀rụ̀': 135,\n",
       " 'ya': 136,\n",
       " 'yi': 137,\n",
       " 'yà': 138,\n",
       " 'yá': 139,\n",
       " 'zitere': 140,\n",
       " 'zutere': 141,\n",
       " 'zụrụ': 142,\n",
       " 'zụ̀tàrà': 143,\n",
       " 'ábiàkwàrà': 144,\n",
       " 'áchíchá': 145,\n",
       " 'áchọ̀tárà': 146,\n",
       " 'áhà': 147,\n",
       " 'áhụ̀': 148,\n",
       " 'áhụ̀rụ̀': 149,\n",
       " 'ájụ́jụ́': 150,\n",
       " 'ákwúkwọ́': 151,\n",
       " 'ákwụ́kwọ́': 152,\n",
       " 'áká': 153,\n",
       " 'ámàaraḿ': 154,\n",
       " 'ébe': 155,\n",
       " 'échìchì': 156,\n",
       " 'édèrè': 157,\n",
       " 'égo': 158,\n",
       " 'émèrè': 159,\n",
       " 'énwò': 160,\n",
       " 'òsìkápá': 161,\n",
       " 'úlọ̀': 162,\n",
       " 'ńnú': 163,\n",
       " 'ńrí': 164,\n",
       " 'ḿ': 165,\n",
       " 'ḿbinye': 166,\n",
       " 'ḿkwà': 167,\n",
       " 'ḿma': 168,\n",
       " 'ḿmańyá': 169,\n",
       " 'ịkwụsị': 170,\n",
       " 'ịwụ': 171,\n",
       " 'ọ': 172,\n",
       " 'ọjọọ': 173,\n",
       " 'ọrụ': 174,\n",
       " 'ọ́': 175,\n",
       " 'ọ́rụ́': 176,\n",
       " 'ụka': 177,\n",
       " 'ụlọ': 178,\n",
       " 'ụnyaahụ': 179,\n",
       " 'ụra': 180,\n",
       " 'ụtọ': 181,\n",
       " 'ụzọ': 182,\n",
       " 'ụ́zọ̀': 183}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((74,), (19,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_lower, y_lower, test_size = 0.2)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34     i wrote the letter\n",
       "16    he knocked the door\n",
       "Name: English Sentence , dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34    START_ édèrè ḿ ákwụ́kwọ́ ozí ahù _END\n",
       "16        START_ ọ kụrụ áká nọnú ụ́zọ̀ _END\n",
       "Name: IGBO SENTENCES , dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['START_', 'áchọ̀tárà', 'ḿ', 'ákwúkwọ́', '_END']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_length = 5\n",
    "tar_length = 9\n",
    "\n",
    "def batch_generator(X,y, batch_size):\n",
    "    while True:\n",
    "        for j in enumerate(0, len(X), batch_size):\n",
    "            #preparing a dummy array of zeros that will be encoded for the corresponding batch_size\n",
    "            encode_input = np.zeros((batch_size, src_length), dtype = np.float32)\n",
    "            decode_input_target = np.zeros((batch_size, tar_length), dtype = np.float32)\n",
    "            decode_target_data = np.zeros((batch_size, tar_length,num_decoder_tokens), dtype = np.float32)\n",
    "            \n",
    "            #encoding the data in batches\n",
    "            for i, (text_input, target) in enumerate(zip(X[j:j+batch_size],y[j:j+batch_size])):\n",
    "                # dealing with each input, target (sentence by sentence in the batch)\n",
    "                for t, word in enumerate(text_input.split()):\n",
    "                    # encode input data\n",
    "                    encode_input[i][t] = input_token_index[word] #get the encoded number for every corresponding word\n",
    "                    \n",
    "                for t, word in enumerate(target_text.split()):\n",
    "                    #encode_target data\n",
    "                    decode_input_target[i][t] = target_token_index[word] # get the docoded number for every corresponding word in the target.\n",
    "                    \n",
    "                    if t > 0:\n",
    "                        # creating an offset of one time step\n",
    "                        # that is excluding the start token\n",
    "                        decode_target_data[i][t-1][target_token_index[word]] = 1\n",
    "            return encoder_input, decode_input_target, decode_target_data\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder-decoder architecture\n",
    "\n",
    "#encoder\n",
    "from keras.layers import Dropout, Input, Embedding, LSTM\n",
    "\n",
    "dimension = 5\n",
    "#Encoder\n",
    "encoder_inputs = Input(shape = (None,))\n",
    "enc_emb = Embedding(num_encoder_tokens, dimension, mask_zero = True)(encoder_inputs)\n",
    "encoder_lstm = LSTM(dimension, return_state = True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "\n",
    "encoder_states = [state_h, state_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder model\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "\n",
    "decoder_inputs = Input(shape = (None,))\n",
    "dec_emb_layer = Embedding(num_decoder_tokens, dimension,mask_zero = True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_lstm = LSTM(dimension, return_sequences = True, return_state = True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state = encoder_states)\n",
    "output = Dense(num_decoder_tokens, activation = 'softmax')(decoder_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer= 'rmsprop', loss = 'categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder at test time\n",
    "\n",
    "#encode the input sequence to get the 'thought vectors'\n",
    "encoder_model = Model(encoder_inputs, encoder_states)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
